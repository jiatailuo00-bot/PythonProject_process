#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSM å¼ºéµå¾ªè¯†åˆ«è„šæœ¬

æ ¹æ® CSM å›å¤å†…å®¹è¯†åˆ«æ˜¯å¦å‘½ä¸­å¼ºéµå¾ªè¯æœ¯:
1. è¯»å–ç‹¬ç«‹ç»´æŠ¤çš„ `force_patterns_config.json` ä¸­çš„å¼ºéµå¾ªè¯æœ¯æ¨¡å¼
2. å¯¹è¾“å…¥æ•°æ®é€æ¡åŒ¹é…ï¼Œè¾“å‡ºå¼ºéµå¾ªæ ‡ç­¾åŠæ¨èè¯æœ¯

ç”¨æ³•ç¤ºä¾‹:
    python detect_csm_force_compliance.py è¾“å…¥æ–‡ä»¶.xlsx --text-column reply
    python detect_csm_force_compliance.py è¾“å…¥æ–‡ä»¶.xlsx --config my_patterns.json

è¾“å‡º:
    - åŸå§‹æ•°æ®é™„åŠ ä»¥ä¸‹å­—æ®µ:
        * å¼ºéµå¾ªè¯†åˆ«ç»“æœ       (æ˜¯ / å¦)
        * å¼ºéµå¾ªè¯†åˆ«è¯æœ¯       (å‘½ä¸­çš„æ ‡å‡†è¯æœ¯æ–‡æœ¬)
        * å¼ºéµå¾ªè¯†åˆ«åˆ†ç±»       (course_overview / module_explanation ç­‰)
        * å¼ºéµå¾ªè¯†åˆ«æ¨¡å¼       (pattern åç§°)
        * å¼ºéµå¾ªè¯†åˆ«å…³é”®è¯     (å‘½ä¸­çš„æ ¸å¿ƒå…³é”®è¯åˆ—è¡¨)
        * å¼ºéµå¾ªè¯†åˆ«ç½®ä¿¡åº¦     (åŒ¹é…å¾—åˆ†, 0-1)
    - ç»“æœé»˜è®¤ä¿å­˜ä¸º `<è¾“å…¥æ–‡ä»¶å>_force_detected.xlsx`
"""

from __future__ import annotations

import argparse
import json
import math
import os
import re
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
from difflib import SequenceMatcher

import pandas as pd

ATTACHMENT_JSON_RE = re.compile(r'\{ *"bucketName"[^{}]*\}', re.IGNORECASE)
MARKDOWN_MEDIA_RE = re.compile(r"\[[^\]]+\]\([^)]+\)")
SQUARE_MEDIA_RE = re.compile(r"\[[^\]]+\.(?:png|jpg|jpeg|gif|pdf|docx?)\]")
COLOR_CODE_RE = re.compile(r"\b\d{1,3};rgb:[0-9a-fA-F/]+\b")

CATEGORY_PRIORITY = {
    "province_analysis": 0,
    "module_explanation": 1,
    "course_overview": 2,
    "follow_up": 3,
    "homework_feedback": 4,
    "general_force_compliance": 5,
    "class_invitation": 6,
}

MODULE_STRICT_TERMS = [
    "æ•°é‡", "æ•°é‡å…³ç³»", "å¸¸è¯†", "èµ„æ–™", "èµ„æ–™åˆ†æ", "åˆ¤æ–­", "å›¾åˆ¤", "å›¾å½¢æ¨ç†",
    "è¨€è¯­", "ç”³è®º", "ä¸»è§‚é¢˜", "æ”¿æ²»ç†è®º", "æ•°å­¦è¿ç®—", "æ—¶æ”¿"
]

MODULE_PHRASE_WHITELIST = [
    "æ•°é‡è¿™ä¸ªæ¨¡å—", "æ•°é‡å…³ç³»ç¡®å®", "æ•°é‡å…³ç³»æ˜¯ä¸ªéš¾ç‚¹", "å¸¸è¯†çŸ¥è¯†ç‚¹", "èµ„æ–™éš¾åº¦ä¸å¤§",
    "è¨€è¯­æœ¬èº«éš¾åº¦ä¸å¤§", "åˆ¤æ–­æ¨¡å—éš¾åº¦", "ä¸»è§‚é¢˜éš¾æ˜“ä¸å¥½è¡¡é‡", "ç”³è®ºå…¶å®å°±æ˜¯",
    "æ”¿æ²»ç†è®ºæ˜¯æ–°å¢çƒ­ç‚¹", "æ—¶æ”¿æ¯”è¾ƒè€ƒéªŒ", "è¿™å‡ ä¸ªæ¨¡å—", "æ¨¡å—ä¸å¤ªå¥½"
]


def strip_attachment_tokens(text: str) -> str:
    """ç§»é™¤å›¾ç‰‡/é™„ä»¶é“¾æ¥ã€è‰²å€¼ç­‰å™ªå£°å†…å®¹"""
    cleaned = MARKDOWN_MEDIA_RE.sub(" ", text)
    cleaned = SQUARE_MEDIA_RE.sub(" ", cleaned)
    cleaned = ATTACHMENT_JSON_RE.sub(" ", cleaned)
    cleaned = COLOR_CODE_RE.sub(" ", cleaned)
    return cleaned


def normalize_text(text: str) -> str:
    """ç»Ÿä¸€å¤„ç†æ–‡æœ¬ä¸­çš„æ¢è¡Œç¬¦ã€é™„ä»¶æ ‡è®°ä¸ç©ºç™½å­—ç¬¦"""
    normalized = text.replace("<newline>", " ")
    normalized = strip_attachment_tokens(normalized)
    normalized = re.sub(r"\s+", " ", normalized).strip()
    return normalized


def extract_candidate_keywords(text: str) -> List[str]:
    """
    åŸºäºæ ‡ç‚¹æ‹†åˆ†ç‰‡æ®µï¼Œç­›é€‰é•¿åº¦>=2çš„ç‰‡æ®µä½œä¸ºå…³é”®è¯ã€‚
    å…³é”®è¯ç”¨äºåç»­çš„å­ä¸²åŒ¹é…ã€‚
    """
    cleaned = strip_attachment_tokens(text.replace("<newline>", " "))
    parts = re.split(r"[ï¼Œ,ã€‚ï¼ï¼Ÿ!?ï¼›;ï¼š:ã€\n\r\t\s()ï¼ˆï¼‰\[\]{}<>ã€Šã€‹â€œâ€\"'Â·â€¦~ï½â¤-]+", cleaned)
    keywords = [p.strip() for p in parts if len(p.strip()) >= 2]
    return keywords


@dataclass
class ScriptPattern:
    """å•æ¡å¼ºéµå¾ªè¯æœ¯æ¨¡å¼"""

    name: str
    category: str
    text: str
    source: str
    keywords: List[str] = field(default_factory=list)
    min_matches: int = 2
    min_ratio: float = 0.45
    template: str = ""
    fuzzy_threshold: float = 0.88
    mandatory_keywords: List[str] = field(default_factory=list)

    def _keyword_match(self, reply_text: str) -> Tuple[float, List[str]]:
        """å…³é”®è¯åŒ¹é…"""
        if not reply_text or not self.keywords:
            return 0.0, []

        matched = [kw for kw in self.keywords if kw and kw in reply_text]
        total = len(self.keywords) or 1
        ratio = len(matched) / total
        return ratio, matched

    def _normalize_for_template(self, text: str) -> str:
        """æ¨¡æ¿ç›¸ä¼¼åº¦å½’ä¸€åŒ–ï¼šå»éå­—æ¯æ•°å­—æ±‰å­—ã€ç»Ÿä¸€å¤§å°å†™ã€æ•°å­—â†’x"""
        if not text:
            return ""
        # æ›¿æ¢<newline>ç­‰
        normalized = text.replace("<newline>", " ")
        # åªä¿ç•™å­—æ¯æ•°å­—æ±‰å­—
        normalized = re.sub(r"[^\w\u4e00-\u9fff]+", " ", normalized, flags=re.UNICODE)
        # æŠŠæ•°å­—æ›¿æ¢ä¸ºx
        normalized = re.sub(r"\d", "x", normalized)
        # å¤šä½™ç©ºæ ¼
        normalized = re.sub(r"\s+", " ", normalized).strip().lower()
        return normalized

    def _fuzzy_match(self, reply_text: str) -> float:
        """æ¨¡æ¿ç›¸ä¼¼åº¦"""
        if not self.template:
            return 0.0

        normalized_reply = self._normalize_for_template(reply_text)
        normalized_template = self._normalize_for_template(self.template)
        if not normalized_reply or not normalized_template:
            return 0.0

        # å¿«é€ŸåŒ…å«åˆ¤æ–­
        if normalized_template in normalized_reply:
            return 1.0
        if normalized_reply in normalized_template:
            return len(normalized_reply) / len(normalized_template)

        return SequenceMatcher(None, normalized_reply, normalized_template).ratio()

    def is_hit(self, reply_text: str) -> Tuple[bool, float, List[str]]:
        """ç»¼åˆå…³é”®è¯ä¸æ¨¡æ¿ç›¸ä¼¼åº¦åˆ¤å®š"""
        keyword_ratio, matched_keywords = self._keyword_match(reply_text)

        keyword_pass = (
            len(matched_keywords) >= self.min_matches or keyword_ratio >= self.min_ratio
        )
        if self.mandatory_keywords:
            keyword_pass = keyword_pass and all(
                mk in matched_keywords for mk in self.mandatory_keywords if mk
            )

        fuzzy_ratio = self._fuzzy_match(reply_text)
        fuzzy_pass = not self.template or fuzzy_ratio >= self.fuzzy_threshold

        if keyword_pass and fuzzy_pass:
            return True, max(keyword_ratio, fuzzy_ratio), matched_keywords

        return False, max(keyword_ratio, fuzzy_ratio), matched_keywords


class CSMForceComplianceDetector:
    """å¼ºéµå¾ªè¯†åˆ«æ ¸å¿ƒé€»è¾‘"""

    def __init__(self, config_path: str):
        self.config_path = config_path
        self.patterns: List[ScriptPattern] = []
        self._load_patterns_from_config()

    def _load_patterns_from_config(self) -> None:
        if not os.path.exists(self.config_path):
            raise FileNotFoundError(
                f"ç¼ºå°‘å¼ºéµå¾ªè¯æœ¯é…ç½®æ–‡ä»¶: {self.config_path}ï¼Œ"
                "è¯·åˆ›å»ºç‹¬ç«‹çš„æ¨¡å¼ç»´æŠ¤æ–‡ä»¶(ä¾‹å¦‚ force_patterns_config.json)ã€‚"
            )

        with open(self.config_path, "r", encoding="utf-8") as f:
            config = json.load(f)

        raw_patterns = config.get("patterns", [])
        if not raw_patterns:
            raise ValueError(f"{self.config_path} ä¸­æœªæ‰¾åˆ° patterns åˆ—è¡¨")

        loaded: List[ScriptPattern] = []
        for idx, pattern_conf in enumerate(raw_patterns, 1):
            text = normalize_text(pattern_conf.get("text", ""))
            if not text:
                continue

            category = pattern_conf.get("category", "").strip()
            if not category:
                continue
            if category == "module_explanation":
                if not any(term in text for term in MODULE_STRICT_TERMS):
                    continue
                if not any(phrase in text for phrase in MODULE_PHRASE_WHITELIST):
                    continue

            min_matches = pattern_conf.get("min_matches")
            min_ratio = pattern_conf.get("min_ratio")

            if "keywords" in pattern_conf:
                keywords = [kw.strip() for kw in pattern_conf["keywords"] if kw.strip()]
            else:
                keywords = sorted(set(extract_candidate_keywords(text)))
            if not keywords:
                continue

            min_matches = int(min_matches) if isinstance(min_matches, (int, float)) else max(2, math.ceil(len(keywords) * 0.35))
            min_ratio = float(min_ratio) if isinstance(min_ratio, (int, float)) else (0.5 if category == "province_analysis" else 0.45)
            template = pattern_conf.get("template", "")
            fuzzy_threshold = float(pattern_conf.get("fuzzy_threshold", 0.88))
            mandatory_keywords = [kw.strip() for kw in pattern_conf.get("mandatory_keywords", []) if kw.strip()]

            loaded.append(
                ScriptPattern(
                    name=pattern_conf.get("name", f"pattern_{idx}"),
                    category=category,
                    text=text,
                    source=pattern_conf.get("source", "custom"),
                    keywords=keywords,
                    min_matches=min_matches,
                    min_ratio=min_ratio,
                    template=template,
                    fuzzy_threshold=fuzzy_threshold,
                    mandatory_keywords=mandatory_keywords,
                )
            )

        if not loaded:
            raise ValueError(f"{self.config_path} ä¸­æœªæˆåŠŸåŠ è½½ä»»ä½•æ¨¡å¼ï¼Œè¯·æ£€æŸ¥é…ç½®å†…å®¹ã€‚")

        # å»é‡ (category + text)
        unique: Dict[Tuple[str, str], ScriptPattern] = {}
        for pattern in loaded:
            unique[(pattern.category, pattern.text)] = pattern

        self.patterns = sorted(unique.values(), key=lambda p: (p.category, p.name))
        print(f"âœ… ä»é…ç½®åŠ è½½å¼ºéµå¾ªæ¨¡å¼ {len(self.patterns)} æ¡ (æ¥æº: {self.config_path})")

    # ------------------------------------------------------------------ #
    # åŒ¹é…é€»è¾‘
    # ------------------------------------------------------------------ #
    def detect_reply(self, reply_text: str) -> Dict[str, object]:
        """
        å¯¹å•æ¡ CSM å›å¤è¿›è¡Œè¯†åˆ«
        è¿”å›: åŒ…å«æ˜¯å¦å‘½ä¸­ã€å‘½ä¸­è¯æœ¯ã€åˆ†ç±»ã€åŒ¹é…å…³é”®è¯ã€å¾—åˆ†ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        if not isinstance(reply_text, str) or not reply_text.strip():
            return self._empty_result()

        normalized = normalize_text(reply_text)
        best_match: Optional[Tuple[ScriptPattern, float, List[str]]] = None

        for pattern in self.patterns:
            is_hit, ratio, matched_keywords = pattern.is_hit(normalized)
            if not is_hit:
                continue

            candidate = (pattern, ratio, matched_keywords)
            if best_match is None:
                best_match = candidate
            else:
                best_pattern, best_ratio, best_keywords = best_match
                if ratio > best_ratio:
                    best_match = candidate
                elif math.isclose(ratio, best_ratio):
                    if len(matched_keywords) > len(best_keywords):
                        best_match = candidate
                    elif len(matched_keywords) == len(best_keywords):
                        current_priority = CATEGORY_PRIORITY.get(pattern.category, 99)
                        best_priority = CATEGORY_PRIORITY.get(best_pattern.category, 99)
                        if current_priority < best_priority:
                            best_match = candidate

        if best_match is None:
            return self._empty_result()

        pattern, ratio, matched_keywords = best_match
        return {
            "is_force": True,
            "force_script": pattern.text,
            "force_category": pattern.category,
            "force_pattern": pattern.name,
            "force_keywords": ", ".join(matched_keywords),
            "force_score": round(ratio, 4),
            "force_source": pattern.source,
        }

    @staticmethod
    def _empty_result() -> Dict[str, object]:
        return {
            "is_force": False,
            "force_script": "",
            "force_category": "",
            "force_pattern": "",
            "force_keywords": "",
            "force_score": 0.0,
            "force_source": "",
        }

    # ------------------------------------------------------------------ #
    # æ•°æ®å¤„ç†
    # ------------------------------------------------------------------ #
    def annotate_dataframe(self, df: pd.DataFrame, text_column: str) -> pd.DataFrame:
        """ä¸º DataFrame é™„åŠ å¼ºéµå¾ªè¯†åˆ«ç»“æœ"""
        if text_column not in df.columns:
            raise KeyError(f"æœªæ‰¾åˆ°å›å¤åˆ— '{text_column}'ï¼Œå½“å‰åˆ—: {list(df.columns)}")

        results = df[text_column].apply(self.detect_reply)
        result_df = pd.json_normalize(results)

        df = df.copy()
        df["å¼ºéµå¾ªè¯†åˆ«ç»“æœ"] = result_df["is_force"].map(lambda x: "æ˜¯" if x else "å¦")
        df["å¼ºéµå¾ªè¯†åˆ«è¯æœ¯"] = result_df["force_script"]
        df["å¼ºéµå¾ªè¯†åˆ«åˆ†ç±»"] = result_df["force_category"]
        df["å¼ºéµå¾ªè¯†åˆ«æ¨¡å¼"] = result_df["force_pattern"]
        df["å¼ºéµå¾ªè¯†åˆ«å…³é”®è¯"] = result_df["force_keywords"]
        df["å¼ºéµå¾ªè¯†åˆ«ç½®ä¿¡åº¦"] = result_df["force_score"]
        df["å¼ºéµå¾ªè¯†åˆ«æ¥æº"] = result_df["force_source"]

        return df


# ---------------------------------------------------------------------- #
# CLI
# ---------------------------------------------------------------------- #
def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="æ ¹æ® CSM å›å¤è¯†åˆ«å¼ºéµå¾ªè¯æœ¯")
    parser.add_argument("input_file",default="1104æŒ–éœ€ä¿®æ”¹æ•°æ®-yyds_pure_improved.xlsx", help="å¾…è¯†åˆ«çš„ Excel/CSV æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--text-column",
        default="å‘é€æ¶ˆæ¯å†…å®¹",
        help="CSM å›å¤æ‰€åœ¨åˆ—å (é»˜è®¤: reply)",
    )
    parser.add_argument(
        "--output-file",
        default="1104æŒ–éœ€ä¿®æ”¹æ•°æ®-yyds_force_detected.xlsx",
        help="ç»“æœè¾“å‡ºè·¯å¾„ (é»˜è®¤: <è¾“å…¥æ–‡ä»¶å>_force_detected.xlsx)",
    )
    parser.add_argument(
        "--config",
        default="force_patterns_config.json",
        help="å¼ºéµå¾ªè¯æœ¯é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: force_patterns_config.json)",
    )
    return parser.parse_args()


def load_dataframe(file_path: str) -> pd.DataFrame:
    ext = os.path.splitext(file_path)[1].lower()
    if ext in [".xlsx", ".xlsm", ".xls"]:
        return pd.read_excel(file_path)
    if ext == ".csv":
        return pd.read_csv(file_path)
    raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")


def guess_default_output(input_path: str) -> str:
    base, _ = os.path.splitext(input_path)
    return f"{base}_force_detected.xlsx"


def main() -> None:
    args = parse_args()

    print("ğŸš€ å¯åŠ¨ CSM å¼ºéµå¾ªè¯†åˆ«")
    print(f"   è¾“å…¥æ–‡ä»¶: {args.input_file}")
    print(f"   å›å¤åˆ—å: {args.text_column}")

    detector = CSMForceComplianceDetector(config_path=args.config)

    df = load_dataframe(args.input_file)
    print(f"ğŸ“– è¯»å–æ•°æ®æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•ï¼Œåˆ—: {list(df.columns)}")

    annotated_df = detector.annotate_dataframe(df, args.text_column)
    total_force = (annotated_df["å¼ºéµå¾ªè¯†åˆ«ç»“æœ"] == "æ˜¯").sum()
    print(f"âœ… å¼ºéµå¾ªè¯†åˆ«å®Œæˆï¼Œå‘½ä¸­ {total_force} æ¡ / {len(annotated_df)}")

    output_file = args.output_file or guess_default_output(args.input_file)
    annotated_df.to_excel(output_file, index=False, engine="openpyxl")
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_file}")


if __name__ == "__main__":
    main()
